{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune Llama-3 with LLaMA Factory\n",
        "\n",
        "Please use a **free** Tesla T4 Colab GPU to run this!\n",
        "\n",
        "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
      ],
      "metadata": {
        "id": "1oHFCsV0z-Jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXON9uxj5_I6",
        "outputId": "18ad5a24-3ff6-4dcf-cb18-c028f95abb7d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Aug  1 21:46:26 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0              57W / 400W |      5MiB / 40960MiB |     55%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "lr7rB3szzhtx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "giM74oK1rRIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cd89d8c-ae8e-4d78-8014-791e33964a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 15962, done.\u001b[K\n",
            "remote: Counting objects: 100% (6698/6698), done.\u001b[K\n",
            "remote: Compressing objects: 100% (271/271), done.\u001b[K\n",
            "remote: Total 15962 (delta 6438), reused 6429 (delta 6427), pack-reused 9264\u001b[K\n",
            "Receiving objects: 100% (15962/15962), 222.36 MiB | 13.87 MiB/s, done.\n",
            "Resolving deltas: 100% (11888/11888), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -q -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check GPU environment"
      ],
      "metadata": {
        "id": "H9RXn_YQnn9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ],
      "metadata": {
        "id": "ZkN-ktlsnrdU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update Identity Dataset"
      ],
      "metadata": {
        "id": "TeYs5Lz-QJYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "NAME = \"Llama-3\"\n",
        "AUTHOR = \"LLaMA Factory\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "ap_fvMBsQHJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5698f5b6-bbb3-4b5a-8bb0-68830d254394"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via LLaMA Board"
      ],
      "metadata": {
        "id": "2QiXcvdzzW3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "id": "YLsdS6V5yUMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via Command Line\n",
        "\n",
        "It takes ~30min for training."
      ],
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                        # do supervised fine-tuning\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  dataset=\"identity,alpaca_en_demo\",             # use alpaca and identity datasets\n",
        "  template=\"llama3\",                     # use llama3 prompt template\n",
        "  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
        "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
        "  output_dir=\"llama3_lora\",                  # the path to save LoRA adapters\n",
        "  per_device_train_batch_size=2,               # the batch size\n",
        "  gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
        "  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
        "  logging_steps=10,                      # log every 10 steps\n",
        "  warmup_ratio=0.1,                      # use warmup scheduler\n",
        "  save_steps=1000,                      # save checkpoint every 1000 steps\n",
        "  learning_rate=5e-5,                     # the learning rate\n",
        "  num_train_epochs=3.0,                    # the epochs of training\n",
        "  max_samples=500,                      # use 500 examples in each dataset\n",
        "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
        "  quantization_bit=4,                     # use 4-bit QLoRA\n",
        "  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
        "  fp16=True,                         # use float16 mixed precision training\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ],
      "metadata": {
        "id": "CS0Qk5OR0i4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fe58a19-d503-4379-a691-70320e1de900"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-08-01 21:54:59.971963: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-08-01 21:54:59.990266: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-01 21:55:00.011967: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-01 21:55:00.018741: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-01 21:55:00.034749: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-08-01 21:55:01.350964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "08/01/2024 21:55:07 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "08/01/2024 21:55:07 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-01 21:55:07,836 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-01 21:55:07,836 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-01 21:55:07,836 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-01 21:55:07,836 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-08-01 21:55:08,236 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "08/01/2024 21:55:08 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
            "08/01/2024 21:55:08 - INFO - llamafactory.data.loader - Loading dataset identity.json...\n",
            "Generating train split: 91 examples [00:00, 5509.98 examples/s]\n",
            "Converting format of dataset: 100% 91/91 [00:00<00:00, 8964.29 examples/s]\n",
            "08/01/2024 21:55:09 - INFO - llamafactory.data.loader - Loading dataset alpaca_en_demo.json...\n",
            "Generating train split: 1000 examples [00:00, 67144.32 examples/s]\n",
            "Converting format of dataset: 100% 500/500 [00:00<00:00, 39654.20 examples/s]\n",
            "Running tokenizer on dataset: 100% 591/591 [00:00<00:00, 1600.49 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 445, 81101, 12, 18, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 445, 81101, 12, 18, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "labels:\n",
            "Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>\n",
            "[INFO|configuration_utils.py:733] 2024-08-01 21:55:10,532 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-08-01 21:55:10,533 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "08/01/2024 21:55:10 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
            "08/01/2024 21:55:10 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
            "[WARNING|quantization_config.py:397] 2024-08-01 21:55:10,562 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "[INFO|modeling_utils.py:3556] 2024-08-01 21:55:10,563 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-08-01 21:55:10,605 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-08-01 21:55:10,609 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-08-01 21:55:13,924 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-08-01 21:55:13,924 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-08-01 21:55:14,209 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-08-01 21:55:14,209 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "08/01/2024 21:55:14 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "08/01/2024 21:55:14 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "08/01/2024 21:55:14 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "08/01/2024 21:55:14 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "08/01/2024 21:55:14 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,v_proj,down_proj,q_proj,up_proj,o_proj,k_proj\n",
            "08/01/2024 21:55:14 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
            "[INFO|trainer.py:642] 2024-08-01 21:55:14,988 >> Using auto half precision backend\n",
            "08/01/2024 21:55:15 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
            "[INFO|trainer.py:2128] 2024-08-01 21:55:15,413 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-08-01 21:55:15,413 >>   Num examples = 591\n",
            "[INFO|trainer.py:2130] 2024-08-01 21:55:15,413 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-08-01 21:55:15,413 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2134] 2024-08-01 21:55:15,413 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2135] 2024-08-01 21:55:15,413 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2136] 2024-08-01 21:55:15,413 >>   Total optimization steps = 222\n",
            "[INFO|trainer.py:2137] 2024-08-01 21:55:15,420 >>   Number of trainable parameters = 20,971,520\n",
            "{'loss': 1.3721, 'grad_norm': 0.7045964002609253, 'learning_rate': 2.173913043478261e-05, 'epoch': 0.14}\n",
            "{'loss': 1.2243, 'grad_norm': 2.0039258003234863, 'learning_rate': 4.347826086956522e-05, 'epoch': 0.27}\n",
            "{'loss': 1.0243, 'grad_norm': 0.7264015078544617, 'learning_rate': 4.98475042744222e-05, 'epoch': 0.41}\n",
            "{'loss': 1.0827, 'grad_norm': 1.1027263402938843, 'learning_rate': 4.910506156279029e-05, 'epoch': 0.54}\n",
            "{'loss': 1.0133, 'grad_norm': 1.3629894256591797, 'learning_rate': 4.7763104379936555e-05, 'epoch': 0.68}\n",
            "{'loss': 1.0055, 'grad_norm': 0.9802199602127075, 'learning_rate': 4.585500840294794e-05, 'epoch': 0.81}\n",
            "{'loss': 1.0707, 'grad_norm': 1.1609805822372437, 'learning_rate': 4.342822968779448e-05, 'epoch': 0.95}\n",
            "{'loss': 0.8445, 'grad_norm': 2.0139942169189453, 'learning_rate': 4.054312439471239e-05, 'epoch': 1.08}\n",
            "{'loss': 0.7177, 'grad_norm': 0.4827647805213928, 'learning_rate': 3.727144767643984e-05, 'epoch': 1.22}\n",
            "{'loss': 0.7951, 'grad_norm': 1.0098525285720825, 'learning_rate': 3.369456906329956e-05, 'epoch': 1.35}\n",
            "{'loss': 0.8142, 'grad_norm': 1.6534184217453003, 'learning_rate': 2.990144873009946e-05, 'epoch': 1.49}\n",
            "{'loss': 0.6996, 'grad_norm': 0.7664632797241211, 'learning_rate': 2.5986424976906322e-05, 'epoch': 1.62}\n",
            "{'loss': 0.6908, 'grad_norm': 1.4672527313232422, 'learning_rate': 2.2046867951027303e-05, 'epoch': 1.76}\n",
            "{'loss': 0.7308, 'grad_norm': 0.5563555359840393, 'learning_rate': 1.8180757964234924e-05, 'epoch': 1.89}\n",
            "{'loss': 0.6038, 'grad_norm': 0.8008321523666382, 'learning_rate': 1.4484248634655401e-05, 'epoch': 2.03}\n",
            "{'loss': 0.5635, 'grad_norm': 1.061604619026184, 'learning_rate': 1.1049275460163999e-05, 'epoch': 2.16}\n",
            "{'loss': 0.5484, 'grad_norm': 0.6935440301895142, 'learning_rate': 7.961269300209159e-06, 'epoch': 2.3}\n",
            "{'loss': 0.5529, 'grad_norm': 1.0181355476379395, 'learning_rate': 5.297031633820193e-06, 'epoch': 2.43}\n",
            "{'loss': 0.54, 'grad_norm': 1.2121793031692505, 'learning_rate': 3.1228244380351602e-06, 'epoch': 2.57}\n",
            "{'loss': 0.4361, 'grad_norm': 0.6420542597770691, 'learning_rate': 1.4927221931831131e-06, 'epoch': 2.7}\n",
            "{'loss': 0.494, 'grad_norm': 0.8943436145782471, 'learning_rate': 4.472670021254899e-07, 'epoch': 2.84}\n",
            "{'loss': 0.5195, 'grad_norm': 1.083727478981018, 'learning_rate': 1.2460271845654569e-08, 'epoch': 2.97}\n",
            "100% 222/222 [08:48<00:00,  2.36s/it][INFO|trainer.py:3478] 2024-08-01 22:04:03,624 >> Saving model checkpoint to llama3_lora/checkpoint-222\n",
            "[INFO|configuration_utils.py:733] 2024-08-01 22:04:04,166 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-08-01 22:04:04,167 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-08-01 22:04:04,364 >> tokenizer config file saved in llama3_lora/checkpoint-222/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-08-01 22:04:04,365 >> Special tokens file saved in llama3_lora/checkpoint-222/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-08-01 22:04:04,865 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 529.4452, 'train_samples_per_second': 3.349, 'train_steps_per_second': 0.419, 'train_loss': 0.7851638436854422, 'epoch': 3.0}\n",
            "100% 222/222 [08:49<00:00,  2.38s/it]\n",
            "[INFO|trainer.py:3478] 2024-08-01 22:04:04,868 >> Saving model checkpoint to llama3_lora\n",
            "[INFO|configuration_utils.py:733] 2024-08-01 22:04:05,394 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-08-01 22:04:05,395 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-08-01 22:04:05,579 >> tokenizer config file saved in llama3_lora/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-08-01 22:04:05,579 >> Special tokens file saved in llama3_lora/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               = 16222836GF\n",
            "  train_loss               =     0.7852\n",
            "  train_runtime            = 0:08:49.44\n",
            "  train_samples_per_second =      3.349\n",
            "  train_steps_per_second   =      0.419\n",
            "[INFO|modelcard.py:449] 2024-08-01 22:04:05,776 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer the fine-tuned model"
      ],
      "metadata": {
        "id": "PVNaC-xS5N40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
        "  template=\"llama3\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  quantization_bit=4,                    # load 4-bit quantized model\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"History has been removed.\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ],
      "metadata": {
        "id": "oh8H9A_25SF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027aa3a0-c7db-419c-df83-1a759174a51f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|tokenization_utils_base.py:2161] 2024-08-01 22:23:03,529 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-01 22:23:03,530 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-01 22:23:03,531 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-08-01 22:23:03,532 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-08-01 22:23:03,925 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/01/2024 22:23:03 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.data.template:Replace eos token: <|eot_id|>\n",
            "[INFO|configuration_utils.py:733] 2024-08-01 22:23:04,167 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-08-01 22:23:04,169 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/01/2024 22:23:04 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llamafactory.model.model_utils.quantization:`quantization_bit` will not affect on the PTQ-quantized models.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/01/2024 22:23:04 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.model_utils.quantization:Loading ?-bit BITSANDBYTES-quantized model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/01/2024 22:23:04 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.patcher:Using KV cache for faster generation.\n",
            "[WARNING|quantization_config.py:397] 2024-08-01 22:23:04,180 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "[INFO|modeling_utils.py:3556] 2024-08-01 22:23:04,182 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-08-01 22:23:04,226 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-08-01 22:23:04,232 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009\n",
            "}\n",
            "\n",
            "[INFO|quantizer_bnb_4bit.py:106] 2024-08-01 22:23:04,437 >> target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
            "[INFO|modeling_utils.py:4364] 2024-08-01 22:23:06,987 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-08-01 22:23:06,989 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-08-01 22:23:07,239 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/3ee63d4b29d5e8dae2b1021856baae7c82eabf59/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-08-01 22:23:07,240 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/01/2024 22:23:07 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.model_utils.attention:Using torch SDPA for faster training and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/01/2024 22:23:08 - INFO - llamafactory.model.adapter - Loaded adapter(s): llama3_lora\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.adapter:Loaded adapter(s): llama3_lora\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/01/2024 22:23:08 - INFO - llamafactory.model.loader - all params: 8,051,232,768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.loader:all params: 8,051,232,768\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\n",
            "\n",
            "User: Hi\n",
            "Assistant: Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?\n",
            "\n",
            "User: 你好\n",
            "Assistant: 您好，我是 Llama-3，一个由 LLaMA Factory 开发的人工智能助手，如何可以为您服务？\n",
            "\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge the LoRA adapter and optionally upload model\n",
        "\n",
        "NOTE: the Colab free version has merely 12GB RAM, where merging LoRA of a 8B model needs at least 18GB RAM, thus you **cannot** perform it in the free version."
      ],
      "metadata": {
        "id": "kTESHaFvbNTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "mcNcHcA4bf4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # use official non-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
        "  template=\"llama3\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  export_dir=\"llama3_lora_merged\",              # the path to save the merged model\n",
        "  export_size=2,                       # the file shard size (in GB) of the merged model\n",
        "  export_device=\"cpu\",                    # the device used in export, can be chosen from `cpu` and `cuda`\n",
        "  #export_hub_model_id=\"your_id/your_model\",         # the Hugging Face hub ID to upload model\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_llama3.json"
      ],
      "metadata": {
        "id": "IMojogHbaOZF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}